{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version:  1.5.0+cpu\n",
      "Tensorflow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Baseline Model Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points to consider in constructing a deep learning model\n",
    "* model\n",
    "* optimizer\n",
    "* epochs\n",
    "* batch size\n",
    "* learning rate\n",
    "* validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X)\n",
    "# model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "activation_list = [\"sigmoid\", \"relu\",\"tanh\",\"softmax\",\"elu\",\"selu\",\n",
    "                   \"deserialize\",\"exponential\",\"get\",\"hard_sigmoid\",\n",
    "                   \"linear\",\"serialize\",\"softplus\",\"softsign\"]\n",
    "\n",
    "loss_list = [\"mse\",\"categorical_crossentropy\", \"sparse_categorical_crossentropy\", tf.keras.losses.BinaryCrossentropy(from_logits=True)]\n",
    "\n",
    "optimizer_list = [tf.keras.optimizers.SGD(lr=learning_rate),tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                 tf.keras.optimizers.Adamax(lr=learning_rate), tf.keras.optimizers.Adadelta(lr=learning_rate),\n",
    "                 tf.keras.optimizers.Adagrad(lr=learning_rate), tf.keras.optimizers.RMSprop(lr=learning_rate)]\n",
    "\n",
    "metrics_list = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "* Simple Multi-Layer Perceptron model\n",
    "* Data from https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "titanic_data = pd.read_csv(\"../data/titanic_train.csv\")\n",
    "X = titanic_data.drop([\"Survived\"], axis = 1).copy()\n",
    "X = X[X.columns[X.dtypes==int]]\n",
    "y = titanic_data[\"Survived\"].copy()\n",
    "\n",
    "# variable setting\n",
    "input_length = len(X.columns)  # number of features\n",
    "n_classes = 1                  # number of classes to be classified\n",
    "\n",
    "# modelling\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units = 5, input_dim = input_length, activation = activation_list[0]))\n",
    "model.add(tf.keras.layers.Dense(units = n_classes, activation = activation_list[1]))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"rmsprop\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 668 samples, validate on 223 samples\n",
      "Epoch 1/10\n",
      "668/668 [==============================] - 1s 911us/sample - loss: 6.0037 - accuracy: 0.6108 - val_loss: 5.6720 - val_accuracy: 0.6323\n",
      "Epoch 2/10\n",
      "668/668 [==============================] - 0s 56us/sample - loss: 6.0037 - accuracy: 0.6108 - val_loss: 5.6720 - val_accuracy: 0.6323\n",
      "Epoch 3/10\n",
      "668/668 [==============================] - 0s 49us/sample - loss: 6.0037 - accuracy: 0.6108 - val_loss: 5.6720 - val_accuracy: 0.6323\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - 0s 50us/sample - loss: 6.0037 - accuracy: 0.6108 - val_loss: 5.6720 - val_accuracy: 0.6323\n",
      "Epoch 5/10\n",
      "668/668 [==============================] - 0s 50us/sample - loss: 6.0037 - accuracy: 0.6108 - val_loss: 5.6720 - val_accuracy: 0.6323\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - 0s 51us/sample - loss: 6.0037 - accuracy: 0.6108 - val_loss: 5.6720 - val_accuracy: 0.6323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb605cd9d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "model.fit(X, y, epochs = 10, batch_size=32, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data \n",
    "\n",
    "* Use vanilla CNN\n",
    "* Data from https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "mnist_data = pd.read_csv(\"../data/mnist_train.csv\")\n",
    "X = mnist_data.iloc[:,1:].values.copy()\n",
    "y = pd.get_dummies(mnist_data.iloc[:,0]).values\n",
    "# y = tf.keras.utils.to_categorical(mnist_data.iloc[:,0], 10)    yields same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable setting\n",
    "n_classes = 10                # number of classes to be classified\n",
    "input_shape = (X.shape[1],)   # input shape: shape of input (tuple) / input_dim = dimension of input (integer) (define only at the first layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model variable setting\n",
    "n_filters = 5\n",
    "kernel_size = (2,2)\n",
    "strides = (1,1)\n",
    "pool_size = (3,3)\n",
    "\n",
    "\n",
    "# modelling\n",
    "model = tf.keras.Sequential()    ## or tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((28,28,1), input_shape = input_shape))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters = n_filters, kernel_size = kernel_size,\n",
    "                                 strides = strides, padding=\"valid\", \n",
    "                                 activation=activation_list[1]))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size = pool_size))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation = activation_list[3]))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer_list[0], metrics = [metrics_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 5)         25        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 9, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 405)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                25984     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 26,659\n",
      "Trainable params: 26,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/10\n",
      "31500/31500 [==============================] - 5s 153us/sample - loss: 2.3939 - accuracy: 0.1107 - val_loss: 2.3011 - val_accuracy: 0.1126\n",
      "Epoch 2/10\n",
      "31500/31500 [==============================] - 4s 141us/sample - loss: 2.3016 - accuracy: 0.1112 - val_loss: 2.3011 - val_accuracy: 0.1126\n",
      "Epoch 3/10\n",
      "31500/31500 [==============================] - 4s 137us/sample - loss: 2.3015 - accuracy: 0.1112 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 4/10\n",
      "31500/31500 [==============================] - 4s 143us/sample - loss: 2.3015 - accuracy: 0.1112 - val_loss: 2.3011 - val_accuracy: 0.1126\n",
      "Epoch 5/10\n",
      "31500/31500 [==============================] - 5s 144us/sample - loss: 2.3015 - accuracy: 0.1112 - val_loss: 2.3010 - val_accuracy: 0.1126\n",
      "Epoch 6/10\n",
      "31500/31500 [==============================] - 4s 135us/sample - loss: 2.3015 - accuracy: 0.1112 - val_loss: 2.3012 - val_accuracy: 0.1126\n",
      "Epoch 7/10\n",
      "31500/31500 [==============================] - 4s 135us/sample - loss: 2.3016 - accuracy: 0.1112 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 8/10\n",
      "31500/31500 [==============================] - 5s 146us/sample - loss: 2.3015 - accuracy: 0.1112 - val_loss: 2.3010 - val_accuracy: 0.1126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb2c18c950>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "model.fit(X, y, epochs = 10, batch_size=32, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "\n",
    "* Use Vanilla RNN/LSTM/GRU\n",
    "* Data from https://www.kaggle.com/rounakbanik/the-movies-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "# the original movies data has been broken down into words with stopwords removed and genres have been cut down to one genres for each movies\n",
    "movie_data = pd.read_csv(\"../data/movies_data.csv\")[[\"original_title\",\"tokens\",\"one_genres\"]]\n",
    "\n",
    "movie_data[\"genre_onehot\"] = pd.Series(pd.get_dummies(movie_data.one_genres).values.tolist())\n",
    "movie_data[\"genre_ind\"] = movie_data[\"genre_onehot\"].apply(lambda x: x.index(1))\n",
    "\n",
    "movie_data = movie_data.drop(movie_data[movie_data.tokens.apply(lambda x: eval(x) == [])].index).reset_index(drop = True)\n",
    "\n",
    "X = movie_data[\"tokens\"].copy()\n",
    "y = np.array(pd.get_dummies(movie_data.one_genres).values.tolist())    # numpy array works better with tensorflow (pandas doesnt work if the elements are arrays)\n",
    "\n",
    "# alternatively you could use tensorflow framework to generate the one-hot vector\n",
    "# y_tokenized = ytokenizer.texts_to_matrix(y)\n",
    "\n",
    "# codes below here turns integers into one-hot\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_categorical = to_categorical(y_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 60 # max length for each sequence\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "padding_methods = [\"pre\",\"post\"]\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_tokenized, padding=padding_methods[0], maxlen=seq_len)\n",
    "\n",
    "# tokenizer.word_index == dictionary of word2index\n",
    "# tokenizer.index_word == dictionary of index2word\n",
    "\n",
    "# align the elements so that they all have equal \"input_length\" (manual padding)\n",
    "# max_seq_length = X.apply(len).max()\n",
    "# X = X.apply(lambda x: x + [0]*(max_seq_length - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding layer (can only be used as the first layer)\n",
    "\"\"\"\n",
    "N = vocab size + 1 (if vocab size = 999, N = 1000)\n",
    "embedding_size = the size of the input\n",
    "input_length = length of the sentence\n",
    "\n",
    "takes input of (batch, input_length)\n",
    "\n",
    "model.output_shape == (None, input_length, embedding_size)\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(N, embedding_size)])\n",
    "\n",
    "model.compile(\"rmsprop\", loss = loss_list[2])\n",
    "\n",
    "embedded = model.predict(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "modified from https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "\n",
    "inner_size = vocab size (or embedded size if embedded)\n",
    "batch_size = number of sequences (data)\n",
    "seq_length = length of each data (sentence)\n",
    "\n",
    "inputs = np.random.random([batch_size, seq_length, inner_size]).astype(np.float32)\n",
    "gru = tf.keras.layers.GRU(num_units)\n",
    "\n",
    "output = gru(inputs)  # The output has shape `[batch_size, num_units]`.\n",
    "\n",
    "gru = tf.keras.layers.GRU(num_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[batch_size, seq_length, inner_size]`.\n",
    "# final_state has shape `[batch_size, num_units]`.\n",
    "whole_sequence_output, final_state = gru(inputs)\n",
    "\"\"\"\n",
    "# variable setting\n",
    "N = len(X.explode().unique()) + 1\n",
    "embedding_size = 500\n",
    "input_length = seq_len\n",
    "n_classes = len(y[0])\n",
    "num_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "rnn_lstm_gru = [tf.keras.layers.LSTM(units = num_units), tf.keras.layers.GRU(units = num_units)]\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(N, embedding_size)])\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(rnn_lstm_gru[1]))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units = n_classes))\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = optimizer_list[5], metrics = [metrics_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31444 samples, validate on 10482 samples\n",
      "Epoch 1/5\n",
      "31444/31444 [==============================] - 45s 1ms/sample - loss: 1.0170 - accuracy: 0.8987 - val_loss: 1.4110 - val_accuracy: 0.8632\n",
      "Epoch 2/5\n",
      "31444/31444 [==============================] - 43s 1ms/sample - loss: 0.7718 - accuracy: 0.9040 - val_loss: 0.5234 - val_accuracy: 0.9266\n",
      "Epoch 3/5\n",
      "31444/31444 [==============================] - 1969s 63ms/sample - loss: 0.4917 - accuracy: 0.9184 - val_loss: 0.5160 - val_accuracy: 0.9332\n",
      "Epoch 4/5\n",
      "31444/31444 [==============================] - 44s 1ms/sample - loss: 0.6214 - accuracy: 0.9088 - val_loss: 1.1077 - val_accuracy: 0.8925\n",
      "Epoch 5/5\n",
      "31444/31444 [==============================] - 43s 1ms/sample - loss: 0.4443 - accuracy: 0.9266 - val_loss: 0.4204 - val_accuracy: 0.9202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdac7185290>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "model.fit(X_padded, y, epochs = 5, batch_size = 1000, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series Data\n",
    "\n",
    "* cryptocurrency data from https://www.kaggle.com/philmohun/cryptocurrency-financial-data#consolidated_coin_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
