{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version:  1.5.0+cpu\n",
      "Tensorflow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Baseline Model Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlearning_rate = \\nloss = \\noptimizer = tf.keras.optimizer.\\nmetircs = \\nn_epochs = \\nbatch_size = \\nval_split =\\nearly_stopping = \\n\\nlayer = tf.keras.layers.Dense(units = 1, input_dim = input_dim / input_shape = input_shape)\\nactivation = tf.keras.layers.Activation(\"\")\\n\\nmodel = tf.keras.Sequential()      # or initialize here with using a list format\\n\\nmodel.add()\\n\\nmodel.compile(loss = loss, optimizer = optimizer, metrics = metrics)\\n\\nmodel.summary()\\n\\nmodel.fit(X, y, epochs = n_epochs, batch_size=batch_size, validation_split=val_split\\n            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=early_stopping, monitor=\"val_loss\")])\\n\\nhistory = model.fit()\\n\\nmodel.predict(X)\\nmodel.predict_classes(X)\\n\\nhistory.history[\"loss\"]\\n\\nmodel.evaluate(X, y)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "learning_rate = \n",
    "loss = \n",
    "optimizer = tf.keras.optimizer.\n",
    "metircs = \n",
    "n_epochs = \n",
    "batch_size = \n",
    "val_split =\n",
    "early_stopping = \n",
    "\n",
    "layer = tf.keras.layers.Dense(units = 1, input_dim = input_dim / input_shape = input_shape)\n",
    "activation = tf.keras.layers.Activation(\"\")\n",
    "\n",
    "model = tf.keras.Sequential()      # or initialize here with using a list format\n",
    "\n",
    "model.add()\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs = n_epochs, batch_size=batch_size, validation_split=val_split\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=early_stopping, monitor=\"val_loss\")])\n",
    "\n",
    "history = model.fit()\n",
    "\n",
    "model.predict(X)\n",
    "model.predict_classes(X)\n",
    "\n",
    "history.history[\"loss\"]\n",
    "\n",
    "model.evaluate(X, y)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "activation_list = [\"sigmoid\", \"relu\",\"tanh\",\"softmax\",\"elu\",\"selu\",\n",
    "                   \"deserialize\",\"exponential\",\"get\",\"hard_sigmoid\",\n",
    "                   \"linear\",\"serialize\",\"softplus\",\"softsign\"]\n",
    "\n",
    "loss_list = [\"mse\",\"categorical_crossentropy\", tf.keras.losses.BinaryCrossentropy(from_logits=True)]\n",
    "\n",
    "optimizer_list = [tf.keras.optimizers.SGD(lr=learning_rate),tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                 tf.keras.optimizers.Adamax(lr=learning_rate), tf.keras.optimizers.Adadelta(lr=learning_rate),\n",
    "                 tf.keras.optimizers.Adagrad(lr=learning_rate), tf.keras.optimizers.RMSprop(lr=learning_rate)]\n",
    "\n",
    "metrics_list = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "* simple Multi-Layer Perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic data from kaggle https://www.kaggle.com/c/titanic\n",
    "titanic_data = pd.read_csv(\"../data/titanic_train.csv\")\n",
    "X = titanic_data.drop([\"Survived\"], axis = 1).copy()\n",
    "X = X[X.columns[X.dtypes==int]]\n",
    "y = titanic_data[\"Survived\"].copy()\n",
    "\n",
    "n = len(X.columns)    # number of features (flattened dimension of the input)\n",
    "num_units = 1\n",
    "input_length = n\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_classes = 1\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units = 5, input_dim = input_length, activation = activation_list[0]))\n",
    "model.add(tf.keras.layers.Dense(units = n_classes, activation = activation_list[1]))\n",
    "\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"rmsprop\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 668 samples, validate on 223 samples\n",
      "Epoch 1/10\n",
      "668/668 [==============================] - 0s 653us/sample - loss: 9.2767 - accuracy: 0.3892 - val_loss: 9.6419 - val_accuracy: 0.3677\n",
      "Epoch 2/10\n",
      "668/668 [==============================] - 0s 43us/sample - loss: 9.2758 - accuracy: 0.3892 - val_loss: 9.6419 - val_accuracy: 0.3677\n",
      "Epoch 3/10\n",
      "668/668 [==============================] - 0s 82us/sample - loss: 9.2752 - accuracy: 0.3892 - val_loss: 9.6419 - val_accuracy: 0.3677\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - 0s 81us/sample - loss: 9.2575 - accuracy: 0.3892 - val_loss: 9.6419 - val_accuracy: 0.3677\n",
      "Epoch 5/10\n",
      "668/668 [==============================] - 0s 75us/sample - loss: 9.2561 - accuracy: 0.3892 - val_loss: 9.6419 - val_accuracy: 0.3677\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - 0s 72us/sample - loss: 9.2552 - accuracy: 0.3892 - val_loss: 9.6419 - val_accuracy: 0.3677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0f501e2590>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 10, batch_size=32, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data \n",
    "\n",
    "* Use vanilla CNN\n",
    "* Data from https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = pd.read_csv(\"../data/mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_data.iloc[:,1:].values.copy()\n",
    "y = pd.get_dummies(mnist_data.iloc[:,0]).values\n",
    "# y = tf.keras.utils.to_categorical(mnist_data.iloc[:,0], 10)    yields same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()    ## or tf.keras.models.Sequential()\n",
    "\n",
    "## Different models\n",
    "\n",
    "n = len(X[0])\n",
    "num_units = 1   # output dimension for the dense layer\n",
    "input_shape = (n,)   # input shape: shape of input (tuple) / input_dim = dimension of input (integer) (define only at the first layer)\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_filters = 5\n",
    "kernel_size = (2,2)\n",
    "strides = (1,1)\n",
    "pool_size = (3,3)\n",
    "# You can also construct the layers in the initialization of the model\n",
    "# model = tf.keras.Sequential([tf.keras.layers.Dense(units = 10, input_shape = input_shape, activation = activation_list[0])])\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((28,28,1), input_shape = input_shape))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters = n_filters, kernel_size = kernel_size,\n",
    "                                 strides = strides, padding=\"valid\", \n",
    "                                 activation=activation_list[1]))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size = pool_size))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation = activation_list[3]))\n",
    "\n",
    "model.compile(loss = loss_list[2], optimizer = optimizer_list[0], metrics = [metrics_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 27, 27, 5)         25        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 9, 9, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9, 9, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 405)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                25984     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 26,659\n",
      "Trainable params: 26,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/10\n",
      "31500/31500 [==============================] - 5s 161us/sample - loss: 0.7402 - accuracy: 0.8297 - val_loss: 0.7305 - val_accuracy: 0.8491\n",
      "Epoch 2/10\n",
      "31500/31500 [==============================] - 5s 167us/sample - loss: 0.7319 - accuracy: 0.8463 - val_loss: 0.7178 - val_accuracy: 0.8744\n",
      "Epoch 3/10\n",
      "31500/31500 [==============================] - 5s 152us/sample - loss: 0.7221 - accuracy: 0.8659 - val_loss: 0.7110 - val_accuracy: 0.8881\n",
      "Epoch 4/10\n",
      "31500/31500 [==============================] - 5s 164us/sample - loss: 0.7178 - accuracy: 0.8745 - val_loss: 0.7088 - val_accuracy: 0.8925\n",
      "Epoch 5/10\n",
      "31500/31500 [==============================] - 5s 167us/sample - loss: 0.7146 - accuracy: 0.8809 - val_loss: 0.7040 - val_accuracy: 0.9022\n",
      "Epoch 6/10\n",
      "31500/31500 [==============================] - 5s 164us/sample - loss: 0.7121 - accuracy: 0.8859 - val_loss: 0.7027 - val_accuracy: 0.9047\n",
      "Epoch 7/10\n",
      "31500/31500 [==============================] - 5s 166us/sample - loss: 0.7099 - accuracy: 0.8903 - val_loss: 0.7020 - val_accuracy: 0.9061\n",
      "Epoch 8/10\n",
      "31500/31500 [==============================] - 5s 165us/sample - loss: 0.7078 - accuracy: 0.8946 - val_loss: 0.6961 - val_accuracy: 0.9178\n",
      "Epoch 9/10\n",
      "31500/31500 [==============================] - 5s 168us/sample - loss: 0.7057 - accuracy: 0.8988 - val_loss: 0.6950 - val_accuracy: 0.9202\n",
      "Epoch 10/10\n",
      "31500/31500 [==============================] - 5s 169us/sample - loss: 0.7042 - accuracy: 0.9018 - val_loss: 0.6945 - val_accuracy: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0f48288350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 10, batch_size=32, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "\n",
    "* Use Vanilla RNN/LSTM/GRU\n",
    "* Data from https://www.kaggle.com/rounakbanik/the-movies-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original movies data has been broken down into words with stopwords removed and genres have been cut down to one genres for each movies\n",
    "movie_data = pd.read_csv(\"../data/movies_data.csv\")[[\"original_title\",\"tokens\",\"one_genres\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data[\"genre_onehot\"] = pd.Series(pd.get_dummies(movie_data.one_genres).values.tolist())\n",
    "movie_data[\"genre_ind\"] = movie_data[\"genre_onehot\"].apply(lambda x: x.index(1))\n",
    "\n",
    "movie_data = movie_data.drop(movie_data[movie_data.tokens.apply(lambda x: eval(x) == [])].index).reset_index(drop = True)\n",
    "\n",
    "X = movie_data[\"tokens\"].copy()\n",
    "y = np.array(pd.get_dummies(movie_data.one_genres).values.tolist())    # numpy array works better with tensorflow (pandas doesnt work if the elements are arrays)\n",
    "\n",
    "# alternatively you could use tensorflow framework to generate the one-hot vector\n",
    "# y_tokenized = ytokenizer.texts_to_matrix(y)\n",
    "\n",
    "# codes below here turns integers into one-hot\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_categorical = to_categorical(y_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 60 # max length for each sequence\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "padding_methods = [\"pre\",\"post\"]\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_tokenized, padding=padding_methods[0], maxlen=seq_len)\n",
    "\n",
    "# tokenizer.word_index == dictionary of word2index\n",
    "# tokenizer.index_word == dictionary of index2word\n",
    "\n",
    "# align the elements so that they all have equal \"input_length\" (manual padding)\n",
    "# max_seq_length = X.apply(len).max()\n",
    "# X = X.apply(lambda x: x + [0]*(max_seq_length - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding layer (can only be used as the first layer)\n",
    "\"\"\"\n",
    "N = vocab size + 1 (if vocab size = 999, N = 1000)\n",
    "embedding_size = the size of the input\n",
    "input_length = length of the sentence\n",
    "\n",
    "takes input of (batch, input_length)\n",
    "\n",
    "model.output_shape == (None, input_length, embedding_size)\n",
    "\"\"\"\n",
    "N = len(X.explode().unique()) + 1\n",
    "embedding_size = 500\n",
    "input_length = seq_len\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(N, embedding_size)])\n",
    "\n",
    "model.compile(\"rmsprop\", loss = loss_list[2])\n",
    "\n",
    "embedded = model.predict(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31444 samples, validate on 10482 samples\n",
      "Epoch 1/5\n",
      "31444/31444 [==============================] - 59s 2ms/sample - loss: 0.2228 - accuracy: 0.9417 - val_loss: 0.1725 - val_accuracy: 0.9428\n",
      "Epoch 2/5\n",
      "31444/31444 [==============================] - 44s 1ms/sample - loss: 0.1488 - accuracy: 0.9468 - val_loss: 0.1674 - val_accuracy: 0.9436\n",
      "Epoch 3/5\n",
      "31444/31444 [==============================] - 47s 1ms/sample - loss: 0.1171 - accuracy: 0.9551 - val_loss: 0.1847 - val_accuracy: 0.9420\n",
      "Epoch 4/5\n",
      "31444/31444 [==============================] - 44s 1ms/sample - loss: 0.0866 - accuracy: 0.9662 - val_loss: 0.1994 - val_accuracy: 0.9405\n",
      "Epoch 5/5\n",
      "31444/31444 [==============================] - 43s 1ms/sample - loss: 0.0562 - accuracy: 0.9784 - val_loss: 0.2340 - val_accuracy: 0.9387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc4dde16990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_units = 64\n",
    "\n",
    "\"\"\"\n",
    "modified from https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "\n",
    "inner_size = vocab size (or embedded size if embedded)\n",
    "batch_size = number of sequences (data)\n",
    "seq_length = length of each data (sentence)\n",
    "\n",
    "inputs = np.random.random([batch_size, seq_length, inner_size]).astype(np.float32)\n",
    "gru = tf.keras.layers.GRU(num_units)\n",
    "\n",
    "output = gru(inputs)  # The output has shape `[batch_size, num_units]`.\n",
    "\n",
    "gru = tf.keras.layers.GRU(num_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[batch_size, seq_length, inner_size]`.\n",
    "# final_state has shape `[batch_size, num_units]`.\n",
    "whole_sequence_output, final_state = gru(inputs)\n",
    "\"\"\"\n",
    "cat_classes = len(y[0])\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(N, embedding_size)])\n",
    "\n",
    "rnn_lstm_gru = [tf.keras.layers.LSTM(units = num_units), tf.keras.layers.GRU(units = num_units)]\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(rnn_lstm_gru[1]))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units = cat_classes))\n",
    "\n",
    "model.compile(loss = loss_list[2], optimizer = optimizer_list[5], metrics = [metrics_list[0]])\n",
    "\n",
    "model.fit(X_padded, y, epochs = 5, batch_size = 1000, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series Data\n",
    "\n",
    "* cryptocurrency data from https://www.kaggle.com/philmohun/cryptocurrency-financial-data#consolidated_coin_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
