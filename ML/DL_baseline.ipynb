{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version:  1.5.0+cpu\n",
      "Tensorflow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Baseline Model Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlearning_rate = \\nloss = \\noptimizer = tf.keras.optimizer.\\nmetircs = \\nn_epochs = \\nbatch_size = \\nval_split =\\nearly_stopping = \\n\\nlayer = tf.keras.layers.Dense(units = 1, input_dim = input_dim / input_shape = input_shape)\\nactivation = tf.keras.layers.Activation(\"\")\\n\\nmodel = tf.keras.Sequential()      # or initialize here with using a list format\\n\\nmodel.add()\\n\\nmodel.compile(loss = loss, optimizer = optimizer, metrics = metrics)\\n\\nmodel.summary()\\n\\nmodel.fit(X, y, epochs = n_epochs, batch_size=batch_size, validation_split=val_split\\n            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=early_stopping, monitor=\"val_loss\")])\\n\\nhistory = model.fit()\\n\\nmodel.predict(X)\\nmodel.predict_classes(X)\\n\\nhistory.history[\"loss\"]\\n\\nmodel.evaluate(X, y)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "learning_rate = \n",
    "loss = \n",
    "optimizer = tf.keras.optimizer.\n",
    "metircs = \n",
    "n_epochs = \n",
    "batch_size = \n",
    "val_split =\n",
    "early_stopping = \n",
    "\n",
    "layer = tf.keras.layers.Dense(units = 1, input_dim = input_dim / input_shape = input_shape)\n",
    "activation = tf.keras.layers.Activation(\"\")\n",
    "\n",
    "model = tf.keras.Sequential()      # or initialize here with using a list format\n",
    "\n",
    "model.add()\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X, y, epochs = n_epochs, batch_size=batch_size, validation_split=val_split\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=early_stopping, monitor=\"val_loss\")])\n",
    "\n",
    "history = model.fit()\n",
    "\n",
    "model.predict(X)\n",
    "model.predict_classes(X)\n",
    "\n",
    "history.history[\"loss\"]\n",
    "\n",
    "model.evaluate(X, y)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "activation_list = [\"sigmoid\", \"relu\",\"tanh\",\"softmax\",\"elu\",\"selu\",\n",
    "                   \"deserialize\",\"exponential\",\"get\",\"hard_sigmoid\",\n",
    "                   \"linear\",\"serialize\",\"softplus\",\"softsign\"]\n",
    "\n",
    "loss_list = [\"mse\",\"categorical_crossentropy\", tf.keras.losses.BinaryCrossentropy(from_logits=True)]\n",
    "\n",
    "optimizer_list = [tf.keras.optimizers.SGD(lr=learning_rate),tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                 tf.keras.optimizers.Adamax(lr=learning_rate), tf.keras.optimizers.Adadelta(lr=learning_rate),\n",
    "                 tf.keras.optimizers.Adagrad(lr=learning_rate), tf.keras.optimizers.RMSprop(lr=learning_rate)]\n",
    "\n",
    "metrics_list = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data \n",
    "\n",
    "* Use vanilla CNN\n",
    "* Data from https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = pd.read_csv(\"../data/mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_data.iloc[:,1:].values.copy()\n",
    "y = pd.get_dummies(mnist_data.iloc[:,0]).values\n",
    "# y = tf.keras.utils.to_categorical(mnist_data.iloc[:,0], 10)    yields same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()    ## or tf.keras.models.Sequential()\n",
    "\n",
    "## Different models\n",
    "\n",
    "n = len(X[0])\n",
    "num_units = 1   # output dimension for the dense layer\n",
    "input_shape = (n,)   # input shape: shape of input (tuple) / input_dim = dimension of input (integer) (define only at the first layer)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# You can also construct the layers in the initialization of the model\n",
    "# model = tf.keras.Sequential([tf.keras.layers.Dense(units = 10, input_shape = input_shape, activation = activation_list[0])])\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units = 5, input_shape = input_shape, activation = activation_list[0]))\n",
    "model.add(tf.keras.layers.Dense(units = 10, input_dim = 5))\n",
    "model.add(tf.keras.layers.Activation(activation_list[1]))\n",
    "\n",
    "\n",
    "model.compile(loss = loss_list[1], optimizer = optimizer_list[0], metrics = [metrics_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 3925      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,985\n",
      "Trainable params: 3,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/10\n",
      "31500/31500 [==============================] - 5s 144us/sample - loss: 6.1840 - accuracy: 0.1168 - val_loss: 6.0367 - val_accuracy: 0.1058\n",
      "Epoch 2/10\n",
      "31500/31500 [==============================] - 3s 104us/sample - loss: 5.1386 - accuracy: 0.1047 - val_loss: 4.9200 - val_accuracy: 0.0895\n",
      "Epoch 3/10\n",
      "31500/31500 [==============================] - 4s 135us/sample - loss: 4.8348 - accuracy: 0.0920 - val_loss: 4.8921 - val_accuracy: 0.0952\n",
      "Epoch 4/10\n",
      "31500/31500 [==============================] - 4s 141us/sample - loss: 4.8237 - accuracy: 0.1124 - val_loss: 4.8894 - val_accuracy: 0.1183\n",
      "Epoch 5/10\n",
      "31500/31500 [==============================] - 4s 117us/sample - loss: 4.8228 - accuracy: 0.1163 - val_loss: 4.8879 - val_accuracy: 0.1185\n",
      "Epoch 6/10\n",
      "31500/31500 [==============================] - 4s 113us/sample - loss: 4.8222 - accuracy: 0.1163 - val_loss: 4.8878 - val_accuracy: 0.1183\n",
      "Epoch 7/10\n",
      "31500/31500 [==============================] - 4s 117us/sample - loss: 4.8204 - accuracy: 0.1176 - val_loss: 4.8850 - val_accuracy: 0.1199\n",
      "Epoch 8/10\n",
      "31500/31500 [==============================] - 4s 138us/sample - loss: 4.7721 - accuracy: 0.1498 - val_loss: 4.7985 - val_accuracy: 0.1814\n",
      "Epoch 9/10\n",
      "31500/31500 [==============================] - 4s 114us/sample - loss: 4.6976 - accuracy: 0.1845 - val_loss: 4.7040 - val_accuracy: 0.1880\n",
      "Epoch 10/10\n",
      "31500/31500 [==============================] - 4s 131us/sample - loss: 4.6287 - accuracy: 0.1828 - val_loss: 4.6491 - val_accuracy: 0.2184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f191c0bf2d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 10, batch_size=32, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "\n",
    "* Use Vanilla RNN/LSTM/GRU\n",
    "* Data from https://www.kaggle.com/rounakbanik/the-movies-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original movies data has been broken down into words with stopwords removed and genres have been cut down to one genres for each movies\n",
    "movie_data = pd.read_csv(\"../data/movies_data.csv\")[[\"original_title\",\"tokens\",\"one_genres\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data[\"genre_onehot\"] = pd.Series(pd.get_dummies(movie_data.one_genres).values.tolist())\n",
    "movie_data[\"genre_ind\"] = movie_data[\"genre_onehot\"].apply(lambda x: x.index(1))\n",
    "\n",
    "movie_data = movie_data.drop(movie_data[movie_data.tokens.apply(lambda x: eval(x) == [])].index).reset_index(drop = True)\n",
    "\n",
    "X = movie_data[\"tokens\"].copy()\n",
    "y = np.array(pd.get_dummies(movie_data.one_genres).values.tolist())    # numpy array works better with tensorflow (pandas doesnt work if the elements are arrays)\n",
    "\n",
    "# alternatively you could use tensorflow framework to generate the one-hot vector\n",
    "# y_tokenized = ytokenizer.texts_to_matrix(y)\n",
    "\n",
    "# codes below here turns integers into one-hot\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_categorical = to_categorical(y_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 60 # max length for each sequence\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_tokenized = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding\n",
    "padding_methods = [\"pre\",\"post\"]\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_tokenized, padding=padding_methods[0], maxlen=seq_len)\n",
    "\n",
    "# tokenizer.word_index == dictionary of word2index\n",
    "# tokenizer.index_word == dictionary of index2word\n",
    "\n",
    "# align the elements so that they all have equal \"input_length\" (manual padding)\n",
    "#max_seq_length = X.apply(len).max()\n",
    "#X = X.apply(lambda x: x + [0]*(max_seq_length - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding layer (can only be used as the first layer)\n",
    "\"\"\"\n",
    "N = vocab size + 1 (if vocab size = 999, N = 1000)\n",
    "embedding_size = the size of the input\n",
    "input_length = length of the sentence\n",
    "\n",
    "takes input of (batch, input_length)\n",
    "\n",
    "model.output_shape == (None, input_length, embedding_size)\n",
    "\"\"\"\n",
    "N = len(X.explode().unique()) + 1\n",
    "embedding_size = 500\n",
    "input_length = max_seq_length\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(N, embedding_size)])\n",
    "\n",
    "model.compile(\"rmsprop\", loss = loss_list[1])\n",
    "\n",
    "embedded = model.predict(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31444 samples, validate on 10482 samples\n",
      "Epoch 1/5\n",
      "31444/31444 [==============================] - 52s 2ms/sample - loss: 8.2248 - accuracy: 0.1193 - val_loss: 8.2592 - val_accuracy: 0.1427\n",
      "Epoch 2/5\n",
      "31444/31444 [==============================] - 44s 1ms/sample - loss: 9.2588 - accuracy: 0.1212 - val_loss: 9.7182 - val_accuracy: 0.1442\n",
      "Epoch 3/5\n",
      "31444/31444 [==============================] - 43s 1ms/sample - loss: 8.8937 - accuracy: 0.1215 - val_loss: 7.1795 - val_accuracy: 0.1429\n",
      "Epoch 4/5\n",
      "31444/31444 [==============================] - 44s 1ms/sample - loss: 7.1117 - accuracy: 0.1264 - val_loss: 7.1269 - val_accuracy: 0.1443\n",
      "Epoch 5/5\n",
      "31444/31444 [==============================] - 45s 1ms/sample - loss: 8.0261 - accuracy: 0.1274 - val_loss: 9.1646 - val_accuracy: 0.1468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1903f4d850>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_units = 64\n",
    "\n",
    "\"\"\"\n",
    "modified from https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "\n",
    "inner_size = vocab size (or embedded size if embedded)\n",
    "batch_size = number of sequences (data)\n",
    "seq_length = length of each data (sentence)\n",
    "\n",
    "inputs = np.random.random([batch_size, seq_length, inner_size]).astype(np.float32)\n",
    "gru = tf.keras.layers.GRU(num_units)\n",
    "\n",
    "output = gru(inputs)  # The output has shape `[batch_size, num_units]`.\n",
    "\n",
    "gru = tf.keras.layers.GRU(num_units, return_sequences=True, return_state=True)\n",
    "\n",
    "# whole_sequence_output has shape `[batch_size, seq_length, inner_size]`.\n",
    "# final_state has shape `[batch_size, num_units]`.\n",
    "whole_sequence_output, final_state = gru(inputs)\n",
    "\"\"\"\n",
    "cat_classes = len(y[0])\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(N, embedding_size)])\n",
    "\n",
    "rnn_lstm_gru = [tf.keras.layers.LSTM(units = num_units), tf.keras.layers.GRU(units = num_units)]\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(rnn_lstm_gru[1]))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units = cat_classes))\n",
    "\n",
    "model.compile(loss = loss_list[1], optimizer = optimizer_list[5], metrics = [metrics_list[0]])\n",
    "\n",
    "model.fit(X_padded, y, epochs = 5, batch_size = 1000, validation_split=0.25\n",
    "            , callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series Data\n",
    "\n",
    "* cryptocurrency data from https://www.kaggle.com/philmohun/cryptocurrency-financial-data#consolidated_coin_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
